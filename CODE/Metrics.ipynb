{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa431bf2",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea3feca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import networkx as nx\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import ast #ast.literal_eval() \n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b7b28",
   "metadata": {},
   "source": [
    "# DATA IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('data_main.csv') #main data\n",
    "ref = pd.read_csv('data_reference.csv') #reference\n",
    "cit = pd.read_csv('data_citation.csv') #citation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581374f",
   "metadata": {},
   "source": [
    "to repeat the same analysis on Authors :\n",
    "- 1) DATA['AUTORI'] = DATA[DATA['AUTORI']!=0].dropna().progress_apply(lambda x: ast.literal_eval(x))\n",
    "- 2) Authors = DATA.explode('AUTORI')[['ego_id','AUTORI']]\n",
    "- obs. you need to retrieve the journal data of the Authors from OpenAlex like that:\n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    Authors['journal_id'] = ''\n",
    "    for i in tqdm(AUTORI_UNICI[AUTORI_UNICI['journal_id']==''].index):\n",
    "        response = requests.get(f'https://api.openalex.org/works?page=1&filter=authorships.author.id:{AUTORI_UNICI[\"AUTORI_ID\"][i]},publication_year:-2019&sort=cited_by_count:desc&per_page=5&mailto=youremail@email.it')\n",
    "        if response.status_code == 200:\n",
    "            cu = response.json()\n",
    "            journal_ids = []\n",
    "            for x in cu.get('results', []):  # Use .get() to avoid KeyError if 'results' is missing\n",
    "                primary_location = x.get('primary_location')\n",
    "                if primary_location is not None:\n",
    "                    source = primary_location.get('source')\n",
    "                    if source is not None and 'id' in source:\n",
    "                        journal_ids.append(source['id'])\n",
    "            Authors.at[i, 'journal_id'] = journal_ids  # Use .at for setting values by label\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for index {i} with status code {response.status_code}\") '''\n",
    "            \n",
    "- obs(2) then with the journal_id is possibile to retrieve the concepts of each journal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33739a9",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423cdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance\n",
    "def calculate_balance(group):\n",
    "    counts = Counter(group)\n",
    "    distribution = [counts[x]/len(group) for x in counts]\n",
    "    return entropy(distribution) / entropy([1/len(counts)]*len(counts))\n",
    "\n",
    "# Calculate Disparity\n",
    "def calculate_disparity(group):\n",
    "    group_set = set(group)\n",
    "    if len(group_set) < 2:\n",
    "        return 0\n",
    "    disparity_sum = 0\n",
    "    for combo in combinations(group_set, 2):\n",
    "        disparity_sum += disparity_matrix.loc[combo[0], combo[1]]\n",
    "    return disparity_sum / (len(group_set) * (len(group_set) - 1))\n",
    "\n",
    "#Calculate Pointwise\n",
    "def avDistPmi(clist):\n",
    "    lista = []\n",
    "    for (u,v) in clist:\n",
    "        try:\n",
    "            lista.append(1-G[u][v]['pmi'])\n",
    "        except:\n",
    "            continue\n",
    "    return np.mean(lista)\n",
    "\n",
    "# Flattens a list of lists into a single list \n",
    "\n",
    "def flattenList(original_list):\n",
    "    ll=[element for sublist in original_list for element in sublist]\n",
    "    return ll\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac99570",
   "metadata": {},
   "source": [
    "# Measure of Interdisciplinary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae40d89",
   "metadata": {},
   "source": [
    "_All measures calculated in the following cells refer to the cit dataframe, i.e. the citations received by each ego_id._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b8413",
   "metadata": {},
   "source": [
    "## Balance, Disparity, Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad770a9",
   "metadata": {},
   "source": [
    "Below is an example of how to calculate balance, disparity and variety using the **citations** received by each publication at the level of journal concepts lvl0.\n",
    "\n",
    "To be used, the dataframe must have the following structure :\n",
    " - ego_id = representing the publication on which Balance, Disparity and Variety are to be obtained\n",
    " - citation_works = or referenced_works representing the individual publication that in this case cited (or was cited in the case of referenced) the paper ego_id\n",
    " - journal_concepts = representing in this case the lvl 0 concept associated with the paper, can also be replaced by individual journal ids or at a more granular level by lvl1 journal concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cafa82e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego_id</th>\n",
       "      <th>citation_works</th>\n",
       "      <th>journal_concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W3105918387</td>\n",
       "      <td>https://openalex.org/W3118344881</td>\n",
       "      <td>Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W3105918387</td>\n",
       "      <td>https://openalex.org/W4226048267</td>\n",
       "      <td>Medicine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ego_id                    citation_works  \\\n",
       "0  https://openalex.org/W3105918387  https://openalex.org/W3118344881   \n",
       "0  https://openalex.org/W3105918387  https://openalex.org/W4226048267   \n",
       "\n",
       "  journal_concepts  \n",
       "0         Medicine  \n",
       "0         Medicine  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit.head(2) #obs. ego_id is repeated for all times it has been cited (citation_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each ego_id get all unique pairs of journal_concepts\n",
    "co_occurrences = cit.groupby('ego_id')['journal_concepts'].apply(lambda x: list(combinations(set(x), 2)))\n",
    "pairs_df = pd.DataFrame([item for sublist in co_occurrences for item in sublist], columns=['Field1', 'Field2'])\n",
    "\n",
    "# Count occurrences of each pair\n",
    "pair_counts = pairs_df.groupby(['Field1', 'Field2']).size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "# Inverse of counts as a basic measure of disparity\n",
    "pair_counts['disparity'] = 1 / pair_counts['counts']\n",
    "\n",
    "# Create the Disparity Matrix\n",
    "fields = cit['journal_concepts'].unique()\n",
    "disparity_matrix = pd.DataFrame(np.zeros((len(fields), len(fields))), index=fields, columns=fields)\n",
    "\n",
    "# Populate the matrix\n",
    "for index, row in pair_counts.iterrows():\n",
    "    field1, field2, disparity = row['Field1'], row['Field2'], row['disparity']\n",
    "    disparity_matrix.loc[field1, field2] = disparity\n",
    "    disparity_matrix.loc[field2, field1] = disparity # Ensure the matrix is symmetrical\n",
    "\n",
    "# Fill diagonal with a high disparity value as a field has no disparity with itself\n",
    "np.fill_diagonal(disparity_matrix.values, np.max(disparity_matrix.values))\n",
    "\n",
    "\n",
    "\n",
    "# Balance\n",
    "balance_df = cit.groupby('ego_id')['journal_concepts'].apply(calculate_balance)\n",
    "# Disparity\n",
    "disparity_df = cit.groupby('ego_id')['journal_concepts'].apply(calculate_disparity)\n",
    "# Variety\n",
    "variety_df = cit.groupby('ego_id')['journal_concepts'].nunique()\n",
    "\n",
    "# Dataframe where for each ego_id is associated a measure of Balance, Disparity and Variety\n",
    "measure_citation_concept = pd.DataFrame({\n",
    "    'Balance': balance_df,\n",
    "    'Disparity': disparity_df,\n",
    "    'Variety': variety_df,})\n",
    "#reset index in order to have ego_id as a column and not an index\n",
    "\n",
    "measure_citation_concept.reset_index(inplace = True, drop = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b693300",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cf7f810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert journal concepts associated with each ego_id into a set to remove duplicates\n",
    "dfList = cit.groupby('ego_id')['journal_concepts'].apply(set).reset_index()\n",
    "\n",
    "# Generate all unique pairs (combinations) of journal concepts for each ego_id.\n",
    "dfList['combinations'] = [list(combinations(test_list, 2)) for test_list in dfList['id_journal']]\n",
    "allEdges = flattenList(list(dfList['combinations']))\n",
    "\n",
    "# Count the occurrence of each unique pair using a Counter, which helps in later calculating weights for the graph edges.\n",
    "dEd = collections.Counter(allEdges)\n",
    "\n",
    "# Initialize an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges to the graph, where keys of dEd are the edges and values are used as weights\n",
    "G.add_edges_from(dEd.keys())\n",
    "\n",
    "# Set the weight for each edge in the graph based on the maximum occurrence of each pair (considering both (u,v) and (v,u) directions).\n",
    "for (u,v) in G.edges():\n",
    "    G[u][v]['weight'] = max(dEd[(u,v)], dEd[(v,u)])\n",
    "\n",
    "# Calculate the total weight of all edges in the graph\n",
    "totW = sum([G[u][v]['weight'] for (u,v) in G.edges()])\n",
    "\n",
    "# Normalize the weight of each edge by the total weight of all edges\n",
    "for (u,v) in G.edges():\n",
    "    G[u][v]['wNorm'] = G[u][v]['weight'] / totW\n",
    "\n",
    "# Calculate the weighted degree of each node, then scale it by 0.5. This pk dict represents the probability of each node\n",
    "pk = dict(G.degree(weight='wNorm'))\n",
    "for i in pk.keys():\n",
    "    pk[i] = 0.5 * pk[i]\n",
    "\n",
    "# Calculate the PMI for each edge in the graph based on the normalized weights and the probabilities of the nodes\n",
    "for (u,v) in G.edges():\n",
    "    pmi = -(np.log2(G[u][v]['wNorm'] / (pk[u] * pk[v]))) / (np.log2(G[u][v]['wNorm']))\n",
    "    G[u][v]['pmi'] = max(pmi, 0)  # Assign PMI value to the edge, ensuring it's non-negative\n",
    "\n",
    "# Calculate the average PMI-based distance for the combinations associated with each ego_i. \n",
    "dfList['pmi_distance_citation'] = dfList['combinations'].progress_apply(lambda x: avDistPmi(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
