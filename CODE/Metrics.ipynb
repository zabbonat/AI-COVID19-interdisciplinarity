{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3521663",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73710cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import networkx as nx\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import ast #ast.literal_eval() \n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc2500",
   "metadata": {},
   "source": [
    "# DATA IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('data_main.csv') #main data\n",
    "ref = pd.read_csv('data_reference.csv') #reference\n",
    "cit = pd.read_csv('data_citation.csv') #citation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada00d08",
   "metadata": {},
   "source": [
    "to repeat the same analysis on Authors :\n",
    "- 1) DATA['AUTORI'] = DATA[DATA['AUTORI']!=0].dropna().progress_apply(lambda x: ast.literal_eval(x))\n",
    "- 2) Authors = DATA.explode('AUTORI')[['ego_id','AUTORI']]\n",
    "-  obs. you need to retrieve the journal data of the Authors from OpenAlex like that:\n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    Authors['journal_id'] = ''\n",
    "    for i in tqdm(AUTORI_UNICI[AUTORI_UNICI['journal_id']==''].index):\n",
    "        response = requests.get(f'https://api.openalex.org/works?page=1&filter=authorships.author.id:{Authors[\"AUTORI\"][i]},publication_year:-2019&sort=cited_by_count:desc&per_page=5&mailto=youremail@email.it')\n",
    "        if response.status_code == 200:\n",
    "            cu = response.json()\n",
    "            journal_ids = []\n",
    "            for x in cu.get('results', []):  # Use .get() to avoid KeyError if 'results' is missing\n",
    "                primary_location = x.get('primary_location')\n",
    "                if primary_location is not None:\n",
    "                    source = primary_location.get('source')\n",
    "                    if source is not None and 'id' in source:\n",
    "                        journal_ids.append(source['id'])\n",
    "            Authors.at[i, 'journal_id'] = journal_ids  # Use .at for setting values by label\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for index {i} with status code {response.status_code}\") '''\n",
    "            \n",
    "- obs(2) then with the journal_id is possibile to retrieve the concepts of each journal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa686855",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance\n",
    "def calculate_balance(group):\n",
    "    counts = Counter(group)\n",
    "    distribution = [counts[x]/len(group) for x in counts]\n",
    "    return entropy(distribution) / entropy([1/len(counts)]*len(counts))\n",
    "\n",
    "# Calculate Disparity\n",
    "def calculate_disparity(group):\n",
    "    group_set = set(group)\n",
    "    if len(group_set) < 2:\n",
    "        return 0\n",
    "    disparity_sum = 0\n",
    "    for combo in combinations(group_set, 2):\n",
    "        disparity_sum += disparity_matrix.loc[combo[0], combo[1]]\n",
    "    return disparity_sum / (len(group_set) * (len(group_set) - 1))\n",
    "\n",
    "# Calculate Pointwise\n",
    "def avDistPmi(clist):\n",
    "    lista = []\n",
    "    for (u,v) in clist:\n",
    "        try:\n",
    "            lista.append(1-G[u][v]['pmi'])\n",
    "        except:\n",
    "            continue\n",
    "    return np.mean(lista)\n",
    "\n",
    "# Flattens a list of lists into a single list \n",
    "\n",
    "def flattenList(original_list):\n",
    "    ll=[element for sublist in original_list for element in sublist]\n",
    "    return ll\n",
    "\n",
    "# Calculate share of AI references\n",
    "def is_any_ai_element_present(x, words_ai):\n",
    "    if any(element in x for element in words_ai) :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8eb82e",
   "metadata": {},
   "source": [
    "# Measure of Interdisciplinary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043ee6b",
   "metadata": {},
   "source": [
    "_All measures calculated in the following cells refer to the cit dataframe, i.e. the citations received by each ego_id._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be4ee9",
   "metadata": {},
   "source": [
    "## Balance, Disparity, Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a89a0",
   "metadata": {},
   "source": [
    "Below is an example of how to calculate balance, disparity and variety using the **citations** received by each publication at the level of journal concepts lvl0.\n",
    "\n",
    "To be used, the dataframe must have the following structure :\n",
    " - ego_id = representing the publication on which Balance, Disparity and Variety are to be obtained\n",
    " - citation_works = or referenced_works representing the individual publication that in this case cited (or was cited in the case of referenced) the paper ego_id\n",
    " - journal_concepts = representing in this case the lvl 0 concept associated with the paper, can also be replaced by individual journal ids or at a more granular level by lvl1 journal concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eae2e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego_id</th>\n",
       "      <th>citation_works</th>\n",
       "      <th>journal_concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W3105918387</td>\n",
       "      <td>https://openalex.org/W3118344881</td>\n",
       "      <td>Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W3105918387</td>\n",
       "      <td>https://openalex.org/W4226048267</td>\n",
       "      <td>Medicine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ego_id                    citation_works  \\\n",
       "0  https://openalex.org/W3105918387  https://openalex.org/W3118344881   \n",
       "0  https://openalex.org/W3105918387  https://openalex.org/W4226048267   \n",
       "\n",
       "  journal_concepts  \n",
       "0         Medicine  \n",
       "0         Medicine  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cit.head(2) #obs. ego_id is repeated for all times it has been cited (citation_works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each ego_id get all unique pairs of journal_concepts\n",
    "co_occurrences = cit.groupby('ego_id')['journal_concepts'].apply(lambda x: list(combinations(set(x), 2)))\n",
    "pairs_df = pd.DataFrame([item for sublist in co_occurrences for item in sublist], columns=['Field1', 'Field2'])\n",
    "\n",
    "# Count occurrences of each pair\n",
    "pair_counts = pairs_df.groupby(['Field1', 'Field2']).size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "# Inverse of counts as a basic measure of disparity\n",
    "pair_counts['disparity'] = 1 / pair_counts['counts']\n",
    "\n",
    "# Create the Disparity Matrix\n",
    "fields = cit['journal_concepts'].unique()\n",
    "disparity_matrix = pd.DataFrame(np.zeros((len(fields), len(fields))), index=fields, columns=fields)\n",
    "\n",
    "# Populate the matrix\n",
    "for index, row in pair_counts.iterrows():\n",
    "    field1, field2, disparity = row['Field1'], row['Field2'], row['disparity']\n",
    "    disparity_matrix.loc[field1, field2] = disparity\n",
    "    disparity_matrix.loc[field2, field1] = disparity # Ensure the matrix is symmetrical\n",
    "\n",
    "# Fill diagonal with a high disparity value as a field has no disparity with itself\n",
    "np.fill_diagonal(disparity_matrix.values, np.max(disparity_matrix.values))\n",
    "\n",
    "\n",
    "\n",
    "# Balance\n",
    "balance_df = cit.groupby('ego_id')['journal_concepts'].apply(calculate_balance)\n",
    "# Disparity\n",
    "disparity_df = cit.groupby('ego_id')['journal_concepts'].apply(calculate_disparity)\n",
    "# Variety\n",
    "variety_df = cit.groupby('ego_id')['journal_concepts'].nunique()\n",
    "\n",
    "# Dataframe where for each ego_id is associated a measure of Balance, Disparity and Variety\n",
    "measure_citation_concept = pd.DataFrame({\n",
    "    'Balance': balance_df,\n",
    "    'Disparity': disparity_df,\n",
    "    'Variety': variety_df,})\n",
    "#reset index in order to have ego_id as a column and not an index\n",
    "\n",
    "measure_citation_concept.reset_index(inplace = True, drop = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed08802",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cf7f810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert journal concepts associated with each ego_id into a set to remove duplicates\n",
    "dfList = cit.groupby('ego_id')['journal_concepts'].apply(set).reset_index()\n",
    "\n",
    "# Generate all unique pairs (combinations) of journal concepts for each ego_id.\n",
    "dfList['combinations'] = [list(combinations(test_list, 2)) for test_list in dfList['id_journal']]\n",
    "allEdges = flattenList(list(dfList['combinations']))\n",
    "\n",
    "# Count the occurrence of each unique pair using a Counter, which helps in later calculating weights for the graph edges.\n",
    "dEd = collections.Counter(allEdges)\n",
    "\n",
    "# Initialize an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges to the graph, where keys of dEd are the edges and values are used as weights\n",
    "G.add_edges_from(dEd.keys())\n",
    "\n",
    "# Set the weight for each edge in the graph based on the maximum occurrence of each pair (considering both (u,v) and (v,u) directions).\n",
    "for (u,v) in G.edges():\n",
    "    G[u][v]['weight'] = max(dEd[(u,v)], dEd[(v,u)])\n",
    "\n",
    "# Calculate the total weight of all edges in the graph\n",
    "totW = sum([G[u][v]['weight'] for (u,v) in G.edges()])\n",
    "\n",
    "# Normalize the weight of each edge by the total weight of all edges\n",
    "for (u,v) in G.edges():\n",
    "    G[u][v]['wNorm'] = G[u][v]['weight'] / totW\n",
    "\n",
    "# Calculate the weighted degree of each node, then scale it by 0.5. This pk dict represents the probability of each node\n",
    "pk = dict(G.degree(weight='wNorm'))\n",
    "for i in pk.keys():\n",
    "    pk[i] = 0.5 * pk[i]\n",
    "\n",
    "# Calculate the PMI for each edge in the graph based on the normalized weights and the probabilities of the nodes\n",
    "for (u,v) in G.edges():\n",
    "    pmi = -(np.log2(G[u][v]['wNorm'] / (pk[u] * pk[v]))) / (np.log2(G[u][v]['wNorm']))\n",
    "    G[u][v]['pmi'] = max(pmi, 0)  # Assign PMI value to the edge, ensuring it's non-negative\n",
    "\n",
    "# Calculate the average PMI-based distance for the combinations associated with each ego_i. \n",
    "dfList['pmi_distance_citation'] = dfList['combinations'].progress_apply(lambda x: avDistPmi(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9250be",
   "metadata": {},
   "source": [
    "# AI References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ai = ['artificial intelligence','deep learning','machine learning',\n",
    "          'convolutional neural','computer vision','convolutional', \n",
    "          'neural network','natural language',\n",
    "          'neural networks',\n",
    "          'neural networking',\n",
    "          'image recognition', 'semantic analysis','unsupervised learning','supervised learning',\n",
    "          'recurrent neural',\n",
    "          'sentiment analysis',\n",
    "          'reinforcement learning','statistical learning','adversarial neural',\n",
    "          'text mining','nlp','pattern recognition',\n",
    "          'object detection','image detection','ai applications','ai application','data mining',\n",
    "          'keras','tensorflow','meta learning','trajectory forecasting','trasnfer learning',\n",
    "          'machine translation','data science','object detection','intelligent machine',\n",
    "          'semi supervised learning','speech recognition','backpropagation','semantic search',\n",
    "          'Abductive logic programming',  'Abductive reasoning',  'Abstract data type',\n",
    "          'Action language','Action model learning', 'Action selection', 'Activation function', \n",
    "          'Adaptive algorithm', 'Adaptiveneuro fuzzy inference system',  'Admissible heuristic',  \n",
    "          'Affective computing',  'Agent architecture','AI accelerator ', 'AI complete', 'AlphaGo',\n",
    "          'Ambient intelligence','Answer set programming', 'Anytime algorithm', \n",
    "          'Application programming interface', 'Approximate string matching', \n",
    "          'Approximation error', 'Argumentation framework', 'Artificial general intelligence', \n",
    "          'Artificial immune system', 'AIML', 'Artificial neural network', \n",
    "          'Association for the Advancement of Artificial Intelligence', 'Asymptotic computational complexity', \n",
    "          'Attributional calculus', 'Augmented reality',  \n",
    "          'Automata theory',  'Automated planning and scheduling',  \n",
    "          'Automated reasoning',  'Autonomic computing', 'Autonomous car', \n",
    "          'Autonomous robot', 'Backpropagation', 'Backpropagation through time', \n",
    "          'Backward chaining', 'Bag of words model', \n",
    "          'Bag of words model in computer vision','Batch normalization',\n",
    "          'Bayesian programming', 'Bees algorithm', 'Behavior informatics', \n",
    "          'Behavior tree',  'Belief desire intention software model',  \n",
    "          'Bias-variance tradeoff',  'Big data',  'Big O notation', \n",
    "          'Binary tree', 'Blackboard system', 'Boltzmann machine', \n",
    "          'Boolean satisfiability problem','Brain technology', \n",
    "          'Branching factor', 'Brute-force search', 'Capsule neural network', \n",
    "          'Case basedreasoning', 'Chatbot', 'Cloud robotics', 'Cluster analysis', \n",
    "          'Cobweb', 'Cognitive architecture', 'Cognitive computing', \n",
    "          'Cognitive science', 'Combinatorial optimization', 'Committee machine', \n",
    "          'Commonsense knowledge', 'Commonsense reasoning', 'Computational chemistry', \n",
    "          'Computational complexity theory', 'Computational creativity', 'Computational cybernetics', \n",
    "          'Computational humor','Computational intelligence', 'Computational learning theory', \n",
    "          'Computational linguistics', 'Computational mathematics',  'Computational neuroscience', \n",
    "          'Computational number theory',  'Computational problem', 'Computational statistics',\n",
    "          'Computer automated design', 'Machine listening','Computer vision', 'Concept drift',\n",
    "          'Connectionism', 'Consistent heuristic', 'Constrained conditional model',\n",
    "          'Constraint logic programming', 'Constraint programming', 'Constructed language', \n",
    "          'Control theory', 'Convolutional neural network', 'Darkforest', 'Dartmouth workshop', \n",
    "          'Data augmentation', 'Data fusion', 'Data integration', 'Data mining', 'Data science', \n",
    "          'Datalog', 'Decision boundary','Decision support system',  'Decision theory',  \n",
    "          'Decision tree learning',  'Declarative programming','Deductive classifier', 'Deep Blue', \n",
    "          'Deep learning', 'DeepMind Technologies', 'Default logic', 'Description logic', \n",
    "          'Developmental robotics', 'Dialogue system', 'Dimensionality reduction', 'Discrete system', \n",
    "          'Distributed artificial intelligence', 'Dynamic epistemic logic', 'Eager learning',\n",
    "          'Ebert test','Echo state network', 'Embodied agent', 'Embodied cognitive science', \n",
    "          'Error driven learning', 'Ensemble averaging ', 'Ethics of artificial intelligence', \n",
    "          'Evolutionary algorithm', 'Evolutionary computation',  'Evolving classification function', \n",
    "          'Existential risk from artificial general intelligence','Expert system', \n",
    "          'Fast and frugal trees', 'Feature extraction', 'Feature learning', 'Feature selection',\n",
    "          'Federated learning', 'First order logic', 'Forward chaining', 'Friendly artificial intelligence', \n",
    "          'Fuzzy control system', 'Fuzzy logic', 'Fuzzy rule', 'Fuzzy set', 'General game playing',\n",
    "          'Generative adversarial network', 'Genetic algorithm', 'Genetic operator', \n",
    "          'Glowworm swarm optimization', 'Graph database', 'Graph theory', 'Graph traversal', \n",
    "          'Halting problem', 'Hyper heuristic', 'IEEE Computational Intelligence Society', \n",
    "          'Incremental learning',  'Inference engine',  'Information integration',\n",
    "          'Information Processing Language', 'Intelligence amplification', 'Intelligence explosion', \n",
    "          'Intelligent agent',  'Intelligent control',  'Intelligent personal assistant', \n",
    "          'Issue tree',  'Junction tree algorithm','Kernel method', 'KL ONE', 'Knowledge acquisition', \n",
    "          'Knowledge-based system', 'Knowledge engineering', 'Knowledge extraction',\n",
    "          'Knowledge Interchange Format', 'Knowledge representation andreasoning', \n",
    "          'Lazy learning', 'Lisp', 'Logic programming', 'Long short term memory', \n",
    "          'Machine vision', 'Markov chain', 'Markov decision process', \n",
    "          'Mathematical optimization', 'Machine learning','Machine listening',\n",
    "          'Machine perception', 'Mechanism design', 'Mechatronics', \n",
    "          'Metabolic network reconstruction and simulation',\n",
    "          'Metaheuristic', 'Model checking', 'Modus ponens', 'Modus tollens','Monte Carlo tree search', \n",
    "          'Multi agent system', 'Multi swarm optimization', 'Mycin', 'Naive Bayes classifier',\n",
    "          'Naive semantics', 'Name binding', 'Named entity recognition', 'Named graph', \n",
    "          'Natural language generation', 'Natural language processing', 'Natural language programming', \n",
    "          'Network motif', 'Neural machine translation', 'Neural Turing machine', 'Neuro fuzzy', \n",
    "          'Neurocybernetics', 'Neuromorphic engineering', 'Nondeterministic algorithm',  \n",
    "          'Nouvelle AI',  'NP completeness','NP hardness', 'Occam s razor', 'Offline learning',\n",
    "          'Online machine learning', 'Ontology learning','OpenAI', 'OpenCog', 'Open Mind Common Sense', \n",
    "          'Partial order reduction', 'Partially observable Markov decision process', \n",
    "          'Particle swarm optimization', 'Pathfinding', 'Pattern recognition', 'Predicate logic', \n",
    "          'Predictive analytics', 'Principal component analysis', 'Principle of rationality', \n",
    "          'Probabilistic programming', 'Prolog', 'Propositional calculus', 'Qualification problem',\n",
    "          'Quantum computing', 'Query language', 'Radial basis function network', 'Random forest', \n",
    "          'Reasoning system', 'Recurrent neural network', 'Region connection calculus', \n",
    "          'Reinforcement learning', 'Reservoir computing','Resource Description Framework', \n",
    "          'Restricted Boltzmann machine', 'Rete algorithm', 'Robot', 'Robotics','Rule-based system', \n",
    "          'Satisfiability', 'Search algorithm', 'Self-management', 'Semantic network', \n",
    "          'Semantic reasoner', 'Semantic query' 'Sensor fusion', 'Separation logic', \n",
    "          'Similarity learning', 'Simulated annealing', 'situated approach',\n",
    "          'Situation calculus', 'SLD resolution','Software',  'Software engineering', \n",
    "          'Spatial-temporal reasoning',  'SPARQL',  'Speech recognition',\n",
    "          'Spiking neural network', 'Statistical classification', 'Statistical relational learning',\n",
    "          'Stochastic optimization', 'Stochastic semantic analysis', \n",
    "          'Stanford Research Institute Problem Solver', \n",
    "          'Subject matter expert', 'Superintelligence',\n",
    "          'Supervised learning', 'Support vector machine', 'Support vector machine',  \n",
    "          'Swarm intelligence',  'Symbolic artificial intelligence',  \n",
    "          'Synthetic intelligence',  'Systemsneuroscience', 'Technological singularity', \n",
    "          'Temporal difference learning', 'Tensor network theory','TensorFlow', \n",
    "          'Theoretical computer science', 'Theory of computation', 'Thompson sampling',\n",
    "          'Time complexity', 'Transhumanism', 'Transition system', 'Tree traversal', \n",
    "          'True quantified Boolean formula', 'Turing machine', 'Turing test',\n",
    "          'Type system', 'Unsupervised learning', 'Vision processing unit', \n",
    "          'Watson', 'Weak AI', 'hidden unit', 'hidden layer']\n",
    "          \n",
    "words_ai = list(set(map(lambda x:x.lower(),words_ai))) #lower and unique \n",
    "\n",
    "\n",
    "\n",
    "#calculate the intensity score (0,1) for AI refererence based on their abstract \n",
    "ref['ai_reference']  = ref['abstract'].apply(lambda x : sum([is_any_ai_element_present(x,words_ai) for x in x])/len(x))\n",
    "\n",
    "#obs you need to have the abstract of each reference "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
